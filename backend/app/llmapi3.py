import asyncio
import os
import json
from dotenv import load_dotenv
import httpx
from fastmcp import Client
from fastmcp.client.transports import SSETransport
from fastapi import FastAPI
from fastapi.responses import StreamingResponse

# ----------------------------------------------------
# é…ç½® (Configuration)
# ----------------------------------------------------
 
PROMPT = """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ•°æ®åº“æŸ¥è¯¢åŠ©æ‰‹ï¼Œæ‹¥æœ‰è°ƒç”¨ SQL å·¥å…·çš„èƒ½åŠ›ã€‚è¯·æ ¹æ®ç”¨æˆ·çš„è‡ªç„¶è¯­è¨€éœ€æ±‚ï¼Œåˆ†æå¹¶ç”Ÿæˆåˆé€‚çš„ SQL æŸ¥è¯¢ï¼Œç„¶åè°ƒç”¨å¯¹åº”çš„å·¥å…·æ‰§è¡Œã€‚
ä¸€ã€ ä½ çš„èƒ½åŠ›
- ä½ å¯ä»¥è°ƒç”¨ä»¥ä¸‹å·¥å…·ï¼š
  1. get_dbSchema_tables_list(): è·å–æ•°æ®åº“ä¸­æ‰€æœ‰å¸¦schemaçš„è¡¨æ ¼åˆ—è¡¨
  2. get_table_definition(table_name: str,schema:Optional[str]=None): è·å–æŒ‡å®šè¡¨çš„ç»“æ„å®šä¹‰ï¼ˆåˆ—åã€æ•°æ®ç±»å‹ç­‰ï¼‰
  3. get_table_data(querysql: str): æ‰§è¡Œ SQL æŸ¥è¯¢å¹¶è¿”å›ç»“æœ
äºŒã€ å·¥ä½œæµç¨‹
1. ç†è§£éœ€æ±‚ï¼šä»”ç»†åˆ†æç”¨æˆ·çš„é—®é¢˜ï¼Œæ˜ç¡®éœ€è¦æŸ¥è¯¢çš„æ•°æ®å†…å®¹å’Œæ¡ä»¶
2. æ£€æŸ¥è¡¨ç»“æ„ï¼ˆå¦‚æœéœ€è¦ï¼‰ï¼š
   - é¦–å…ˆè°ƒç”¨ `get_dbSchema_tables_list()` è·å–æ‰€æœ‰è¡¨å
   - ç„¶åè°ƒç”¨ `get_table_definition()` è·å–ç›¸å…³è¡¨çš„ç»“æ„ä¿¡æ¯
3. ç”Ÿæˆ SQLï¼šæ ¹æ®è¡¨ç»“æ„å’Œç”¨æˆ·éœ€æ±‚ï¼Œç”Ÿæˆå‡†ç¡®çš„ SQL æŸ¥è¯¢è¯­å¥
4. æ‰§è¡ŒæŸ¥è¯¢ï¼šè°ƒç”¨ `get_table_data()` æ‰§è¡Œ SQL æŸ¥è¯¢
5. æ•´ç†ç»“æœï¼šå°†æŸ¥è¯¢ç»“æœæ•´ç†æˆè‡ªç„¶è¯­è¨€å›ç­”ç”¨æˆ·
ä¸‰ã€ æ€è€ƒè¿‡ç¨‹
åœ¨å›ç­”ç”¨æˆ·ä¹‹å‰ï¼Œè¯·å…ˆæ€è€ƒï¼š
- ç”¨æˆ·çš„é—®é¢˜éœ€è¦å“ªäº›æ•°æ®ï¼Ÿ
- è¿™äº›æ•°æ®å¯èƒ½å­˜å‚¨åœ¨å“ªäº›è¡¨ä¸­ï¼Ÿ
- æˆ‘æ˜¯å¦çŸ¥é“è¿™äº›è¡¨çš„ç»“æ„ï¼Ÿå¦‚æœä¸çŸ¥é“ï¼Œéœ€è¦å…ˆè°ƒç”¨å·¥å…·è·å–
- SQL æŸ¥è¯¢éœ€è¦å“ªäº›æ¡ä»¶ã€èšåˆå‡½æ•°æˆ–è¿æ¥æ“ä½œï¼Ÿ
- å¦‚ä½•ç¡®ä¿ SQL æŸ¥è¯¢çš„å®‰å…¨æ€§å’Œæ•ˆç‡ï¼Ÿ
å››ã€ æ³¨æ„äº‹é¡¹
- å¿…é¡»ä½¿ç”¨å·¥å…·ï¼šæ‰€æœ‰æ•°æ®åº“æ“ä½œå¿…é¡»é€šè¿‡è°ƒç”¨æä¾›çš„å·¥å…·å®Œæˆï¼Œä¸å¾—ç›´æ¥å›ç­”å‡è®¾æ€§ç»“æœ
- è¡¨åå’Œåˆ—åï¼šä¸¥æ ¼ä½¿ç”¨ä» `get_table_definition()` è·å–çš„å®é™…è¡¨åå’Œåˆ—å
- SQL è¯­æ³•ï¼šç”Ÿæˆçš„ SQL å¿…é¡»ç¬¦åˆå½“å‰æ•°æ®åº“ç±»å‹è¯­æ³•è§„èŒƒ
- å‚æ•°åŒ–æŸ¥è¯¢ï¼šé¿å… SQL æ³¨å…¥é£é™©ï¼Œæ­£ç¡®å¤„ç†ç”¨æˆ·è¾“å…¥çš„å‚æ•°
- é”™è¯¯å¤„ç†ï¼šå¦‚æœå·¥å…·è°ƒç”¨å¤±è´¥æˆ–è¿”å›é”™è¯¯ï¼Œè¯·ä¼˜é›…åœ°å¤„ç†å¹¶å‘ç”¨æˆ·è§£é‡Š
- ç»“æœæ ¼å¼ï¼šå°†æŸ¥è¯¢ç»“æœä»¥æ¸…æ™°æ˜“è¯»çš„æ ¼å¼å‘ˆç°ç»™ç”¨æˆ·
 """
load_dotenv()
API_KEY = os.getenv('OAI_API_KEY')
SSE_URL = os.getenv('SSE_URL', 'http://localhost:19068/sse')
LLM_API = os.getenv('BASE_URL', "https://api.siliconflow.cn/v1/chat/completions")
# æ˜¾å¼è®¾ç½®ç¬¬ä¸€æ¬¡LLMè°ƒç”¨çš„è¶…æ—¶æ—¶é—´ï¼Œé»˜è®¤ä¸º10ç§’
FIRST_PASS_TIMEOUT = float(os.getenv('FIRST_PASS_TIMEOUT', 10.0)) 
MODEL = os.getenv('MODEL', 'GPT-4o')
# Header Information
HEADERS = {
    "Authorization": f"Bearer {API_KEY}",
    "Content-Type": "application/json"
}

# ----------------------------------------------------
# FastAPI Application Initialization
# ----------------------------------------------------
app = FastAPI(
    title="Streaming Database Inspector API (LLM/Tool Integration)",
    description="A FastAPI application demonstrating StreamingResponse (SSE) with LLM and fastmcp tool execution."
)

# ----------------------------------------------------
# Auxiliary Functions
# ----------------------------------------------------
def extract_tool_result(result):
    """Attempt to extract JSON content from fastmcp ClientCallResult"""
    try:
        if result.content:
            raw_text = result.content[0].text
            return json.loads(raw_text)
    except Exception as e:
        print("Failed to parse tool result:", e)
    return str(result)  # Fallback

async def function_calling_stream(messages, tools):
    """
    Asynchronously calls LLM for the second interaction and streams the final reply.
    """
    payload = {
        "model": MODEL,
        "messages": messages,
        "tools": tools,
        "tool_choice": "auto",
        "stream": True  # Key: Enable streaming response
    }

    # ä½¿ç”¨å…¨å±€å®¢æˆ·ç«¯ï¼Œå®ƒçš„é»˜è®¤è¶…æ—¶ä¸º 60.0 ç§’ï¼Œé€‚åˆè¾ƒé•¿çš„æµå¼å“åº”
    async with httpx.AsyncClient(timeout=60.0) as http:
        async with http.stream("POST", LLM_API, headers=HEADERS, json=payload) as response:
            response.raise_for_status()
            async for chunk in response.aiter_text():
                if not chunk.strip():
                    continue
                # Parse SSE format chunk (OpenAI streaming response is JSON fragments)
                for line in chunk.splitlines():
                    line = line.strip()
                    if line.startswith("data: "):
                        data = line[6:]  # Remove "data: " prefix
                        if data == "[DONE]":
                            return
                        try:
                            json_data = json.loads(data)
                            # Extract content fragment (Markdown format)
                            content = json_data.get("choices", [{}])[0].get("delta", {}).get("content", "")
                            if content:
                                yield content  # Stream out each fragment
                        except json.JSONDecodeError:
                            continue

# ----------------------------------------------------
# Core Logic Function (Asynchronous Generator)
# ----------------------------------------------------
async def mcp_main(question: str = ''):
    """
    Processes user query, returns content in Markdown format as a stream.
    Returns: Asynchronous generator, outputs Markdown string chunk by chunk.
    """
    if not API_KEY:
        yield "{\"error\": \"OAI_API_KEY not set in environment.\"}"
        return

    transport = SSETransport(SSE_URL)
    print(f"Using SSE Transport URL: {SSE_URL}")

    # å®¢æˆ·ç«¯è¶…æ—¶è®¾ç½®ä¸º 60.0 ç§’ï¼Œä½†ç¬¬ä¸€æ¬¡ POST è¯·æ±‚ä¼šä½¿ç”¨æ›´çŸ­çš„ FIRST_PASS_TIMEOUT
    async with Client(transport) as client, httpx.AsyncClient(timeout=60.0) as http:
        try:
            # 1. Tool discovery
            # yield "data: ### ğŸš€ åˆå§‹åŒ–å·¥å…·ç¯å¢ƒ...\n\n\n" # Yield header in SSE format
            await client.ping()
            tool_defs = await client.list_tools()

            # Convert tool definition to LLM recognizable format
            tools = []
            for tool in tool_defs:
                tools.append({
                    "type": "function",
                    "function": {
                        "name": tool.name,
                        "description": tool.description,
                        "parameters": tool.inputSchema
                    }
                })
            
            # Yield discovery message in SSE format
            # yield f"data: âœ… å‘ç° {len(tools)} ä¸ªå¯ç”¨å·¥å…·ã€‚å¼€å§‹ LLM å†³ç­– ({FIRST_PASS_TIMEOUT}s è¶…æ—¶)....\n\n\n"
            await asyncio.sleep(0.5)

            # 2. First LLM call: Decision making (non-streaming)
            messages = [{"role":"system","content": PROMPT},{"role": "user", "content": question}]
            payload = {
                "model": MODEL,
                "messages": messages,
                "tools": tools,
                "tool_choice": "auto",
                "stream": False  # First call is non-streaming for quick decision
            }

            # æ˜ç¡®è®¾ç½®è¶…æ—¶ï¼Œä»¥ä¿è¯å¿«é€Ÿå†³ç­–
            response = await http.post(
                LLM_API, 
                headers=HEADERS, 
                json=payload,
                timeout=FIRST_PASS_TIMEOUT 
            )
            print(f"LLM ç¬¬ä¸€æ¬¡è¯·æ±‚çŠ¶æ€ç  : {response.status_code}")
            if response.status_code >= 400:
                print(f"LLM ç¬¬ä¸€æ¬¡è¯·æ±‚é”™è¯¯å“åº” (First LLM request error response): {response.text}")
            
            response.raise_for_status()
            
            response_text = response.text # Keep response_text for parsing

            try:
                # å°è¯• JSON è§£ç 
                data = json.loads(response_text)
            except json.JSONDecodeError as e:
                # æ•è· JSON è§£ç é”™è¯¯ï¼Œå¹¶æ‰“å°åŸå§‹å“åº”æ–‡æœ¬
                print("!!! JSON DECODE ERROR (Status 200) !!!")
                print(f"é”™è¯¯ç±»å‹: {type(e).__name__}, æ¶ˆæ¯: {str(e)}")
                print("åŸå§‹å“åº”æ–‡æœ¬:")
                raw_text = response_text
                print(raw_text[:500] + "..." if len(raw_text) > 500 else raw_text)
                # é‡æ–°æŠ›å‡ºå¼‚å¸¸ï¼Œç”±å¤–éƒ¨ handler å¤„ç†
                raise e 

            try:
                choice = data["choices"][0]
                assistant_message = choice["message"]
                print(f"LLM å›å¤å†…å®¹: {assistant_message}")
                tool_calls = assistant_message.get("tool_calls")
            except (IndexError, KeyError) as e:
                # æ•è·ç»“æ„è®¿é—®é”™è¯¯ï¼Œå¹¶æ‰“å°æ¥æ”¶åˆ°çš„ JSON æ•°æ®
                print("==============================================")
                print("!!! UNEXPECTED JSON STRUCTURE (Status 200) !!!")
                print(f"é”™è¯¯ç±»å‹: {type(e).__name__}, æ¶ˆæ¯: {str(e)}")
                print("JSON DATA (è¯·æ£€æŸ¥ 'choices' æˆ– 'message' é”®/ç´¢å¼•):")
                print(json.dumps(data, indent=2, ensure_ascii=False))
                print("==============================================")
                # é‡æ–°æŠ›å‡ºå¼‚å¸¸
                raise e

            # 3. If LLM replies directly (no tool call)
            if not tool_calls:
                direct_response = assistant_message.get("content", "æœªèƒ½è·å– LLM å›å¤")
                await asyncio.sleep(0.05) # Yield header first
                # Stream the direct response line by line, enforced SSE format
                for line in direct_response.split('\n'):
                    if line:
                        yield f"data: {line}\n\n"
                        await asyncio.sleep(0.1) # Simulate noticeable flow
                return 
            await asyncio.sleep(0.5)

            # Add tool call request to message history
            messages.append(assistant_message)
            # Execute tool calls and output intermediate results
            tool_result_content = ""
            for call in tool_calls:
                tool_name = call["function"]["name"]
                try:
                    arguments = json.loads(call["function"]["arguments"])
                except json.JSONDecodeError:
                    error_msg = f"âŒ å·¥å…·è°ƒç”¨å‚æ•°è§£æé”™è¯¯ï¼š{call['function']['arguments']}\n\n"
                    print(error_msg)
                    yield f"data: {error_msg}\n\n" # Yield error in SSE format
                    continue
                await asyncio.sleep(0.3)
                # Call MCP tool
                result = await client.call_tool(tool_name, arguments)
                tool_result_content = extract_tool_result(result)
                await asyncio.sleep(0.5)
                # Add tool result to message history
                messages.append({
                    "role": "tool",
                    "tool_call_id": call["id"],
                    "content": json.dumps(tool_result_content, ensure_ascii=False)
                })

            async for chunk in function_calling_stream(messages, tools):
                yield f"data: {chunk}\n\n"
                # await asyncio.sleep(0.02)  # Control streaming speed

        except httpx.TimeoutException:
            # ä¸“é—¨æ•è·è¶…æ—¶é”™è¯¯
            error_msg = f" âŒ å¤„ç†å¤±è´¥ - è¶…æ—¶è¿æ¥ LLM API ({LLM_API}) è¶…æ—¶ï¼Œå·²è¶…è¿‡ {FIRST_PASS_TIMEOUT} ç§’ã€‚\n"
            print(error_msg)
            error_msg += "è¯·æ£€æŸ¥ç½‘ç»œè¿æ¥ã€API å¯†é’¥é¢åº¦æˆ–å°è¯•å¢åŠ  `FIRST_PASS_TIMEOUT`ã€‚\n"
            yield f"data: {error_msg}\n\n"
        except Exception as e:
            # æ•è·å…¶ä»–æ‰€æœ‰å¼‚å¸¸
            error_msg = f" âŒ å¤„ç†å¤±è´¥ - å‘ç”Ÿæ„å¤–é”™è¯¯ï¼š`{type(e).__name__}: {str(e)}`\n"
            print(error_msg)
            yield f"data: {error_msg}\n\n"
            